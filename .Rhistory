library(tm)
library(plyr)
library(openNLP)
require("NLP")
library(SnowballC)
#Load the corpus
allFiles = DirSource("reformattedPostings","CP1252")
corp = Corpus(allFiles,readerControl = list(language="en"))
#preprocessing
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeWords, stopwords("english"))
corp <- tm_map(corp, stripWhitespace)
corp <- tm_map(corp, stemDocument)
tdm = TermDocumentMatrix(corp)
tdm
inspect(tdm[1:10,1:10])
inspect(tdm[1000:1010,1000:1010])
inspect(tdm[15000:15010,15000:15010])
inspect(tdm[35000:35010,35000:35010])
inspect(tdm[135000:135010,35000:35010])
inspect(tdm[155000:155010,35000:35010])
inspect(tdm[55000:55010,35000:35010])
tdm
library(tm)
library(plyr)
library(openNLP)
require("NLP")
library(SnowballC)
library(RWeka)
install.packages("RWeka")
library(RWeka)
library(tm)
library(plyr)
library(openNLP)
require("NLP")
library(SnowballC)
library(RWeka)
#initialize openNLP fcns to do part of speech / sentence tagging
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
#Load the corpus
allFiles = DirSource("reformattedPostings","CP1252")
corp = Corpus(allFiles,readerControl = list(language="en"))
dtm = DocumentTermMatrix(corp)
findAssocs(dtm,"GDP",0.001)
findAssocs(dtm,"Cook",0.001)
findAssocs(dtm,"Cook",0.00000000000000001)
findAssocs(dtm,"Cook",0)
findAssocs(dtm,"Apple",0)
dtm$Terms[1]
dtm$Terms
dtm
inspect(dtm[1:10,1:10])
inspect(dtm[1:10,1000:1010])
findAssocs(dtm,"entirely",0)
findAssocs(dtm,"entirely",0.1)
findAssocs(dtm,"GDP",0.1)
corp <- tm_map(corp, stripWhitespace)
findAssocs(dtm,"GDP",0.1)
dtm[[1]]
names(dtm)[1]
names(dtm)[2]
names(dtm)[6]
dtm[6]$ Terms
dtm[6]$Terms
dtm[6][1]
dtm[6]
names(dtm[6][1])
names(dtm[6])
names(dtm)[6]
names(dtm)
dtm$dimnames
dtm$dimnames[1]
dtm$dimnames[2]
dtm$dimnames[2][1:10]
dtm$dimnames[2,1:10]
dtm$dimnames[2][1]
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeWords, stopwords("english"))
corp <- tm_map(corp, stripWhitespace)
corp <- tm_map(corp, stemDocument)
corp <- tm_map(corp, stripWhitespace)
dtm = DocumentTermMatrix(corp)
dtm$dimnames[2][1]
dtm$dimnames[2][1][1000:1050]
tok = dtm$dimnames[2][1]
tok[1]
tok$Terms
tok$Terms[1]
tok$Terms[1000]
tok$Terms[10000]
tok$Terms[100000]
tok$Terms[1000000]
tok$Terms[100090]
tok$Terms[100890]
tok$Terms[100891]
tok$Terms[100890:101890]
findAssocs(corp,"gdp",0)
findAssocs(dtm,"gdp",0)
findAssocs(dtm,"gdp",0.2)
findAssocs(dtm,"tim cook",0.2)
findAssocs(dtm,"tim\ cook",0.2)
findAssocs(dtm,"Tim\ Cook",0.2)
findAssocs(dtm,"Cook",0.2)
findAssocs(dtm,"cook",0.2)
library(tm)
library(plyr)
library(openNLP)
require("NLP")
library(SnowballC)
library(RWeka)
#initialize openNLP fcns to do part of speech / sentence tagging
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
#Load the corpus
allFiles = DirSource("reformattedPostings","CP1252")
corp = Corpus(allFiles,readerControl = list(language="en"))
CustomTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
dtm <- DocumentTermMatrix(corp, control = list(tokenize = CustomTokenizer))
library(RWeka)
CustomTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
dtm <- DocumentTermMatrix(corp, control = list(tokenize = CustomTokenizer))
library(RWeka)
dtm <- DocumentTermMatrix(corp, control = list(tokenize = CustomTokenizer))
